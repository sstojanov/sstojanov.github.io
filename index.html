<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-JZQDMRY8W2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag('js', new Date());
      gtag('config', 'G-JZQDMRY8W2');
    </script>

    <title>Stefan Stojanov</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- Local CSS (kept as-is) -->
    <link rel="stylesheet" href="bootstrap/css/bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="main.css" />

    <!-- Google Font -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" />
  </head>
  <body>
    <!-- Local JS (kept as-is) -->
    <script src="jquery-3.5.1.min.js"></script>
    <script src="bootstrap/js/bootstrap.min.js"></script>

    <!-- Hero / Header -->
    <div class="container-md my-3 border py-4">
      <div class="row align-items-center justify-content-center">
        <div class="col-md-3 px-4">
          <div class="text-center">
            <img src="img/me.jpg" class="img-fluid rounded-circle pb-2" alt="Portrait of Stefan Stojanov" />

            <div class="text-center-fluid">
              <h2 style="margin-bottom: 0">Stefan Stojanov</h2>
              <p style="margin-bottom: 0">stojanov@stanford.edu</p>
              <p style="margin-bottom: 0">
                <a href="https://twitter.com/sstj389">
                  <img src="img/icons/twitter-svgrepo-com.svg" height="25" alt="Twitter" />
                </a>
                <a href="https://github.com/sstojanov/">
                  <img src="img/icons/github-svgrepo-com.svg" height="25" alt="GitHub" />
                </a>
                <a href="https://scholar.google.com/citations?user=XC_WricAAAAJ&amp;hl=en">
                  <img src="img/icons/google-scholar-svgrepo-com.svg" height="25" alt="Google Scholar" />
                </a>
                <a href="https://www.linkedin.com/in/sstojanov/">
                  <img src="img/icons/linkedin-svgrepo-com.svg" height="25" alt="LinkedIn" />
                </a>
                <a href="stefan_stojanov_cv.pdf">
                  <img src="img/icons/cv-file-interface-symbol-svgrepo-com.svg" height="25" alt="CV" />
                </a>
              </p>
            </div>
          </div>
        </div>

        <div class="col-md-7 px-4 py-4">
          <div class="text-center">
            <p align="justify">
              I am an applied scientist at Amazon, where I work on video understanding.
            </p>

            <p align="justify">
              Previously I was a postdoctoral researcher in the <a href="https://svl.stanford.edu/">Stanford Vision and Learning Lab</a> and <a href="https://neuroailab.stanford.edu/">Stanford NeuroAILab</a>, advised by <a href="https://jiajunwu.com/">Jiajun Wu</a> and <a href="http://stanford.edu/~yamins/">Dan Yamins</a>. At Stanford, I worked on developing video foundation models and fine-grained object representations. I was fortunate to be supported by a fellowship from <a href="https://hai.stanford.edu/">Stanford HAI</a>. I received my PhD at the Georgia Institute of Technology advised by <a href="https://rehg.org/">James Rehg</a>, where I worked on continual, self-supervised and low-shot learning, and 3D object shape reconstruction.
            </p>

            <p align="justify">
              My main research interests are in computer vision and machine
              learning. I focus on the intersection of 3D vision,
              self-supervision, data synthesis, and video-based learning. My
              work explores how these areas can complement each other to
              develop systems capable of efficiently learning rich, granular
              representations of the physical world.
            </p>

            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="container-md p-3 my-3 border">
      <h2 class="text-center">Publications & Preprints</h2>

      <div class="publication my-4">
        <div class="row justify-content-center align-items-center">
          <div class="col-auto p-0">
            <img src="img/pub/kim_taming_2025.gif" class="img-fluid" alt="" style="max-width: 150px" />
          </div>
          <div class="col-8">
            <p><b>Taming Generative Video Models for Zero-shot Optical Flow Extraction</b></p>
            <p>A variety of frozen video models can be counterfactually prompted to extract motion in videos.</p>
            <p>
              <a href="https://sekim12.github.io/">Seungwoo&nbsp;Kim*</a>, 
              <a href="https://awwkl.github.io/">Khai&nbsp;Loong&nbsp;Aw*</a>,
              <a href="https://klemenkotar.github.io/">Klemen&nbsp;Kotar*</a>, 
              <a href="https://ceyzaguirre4.github.io/">Cristobal&nbsp;Eyzaguirre</a>,
              <a href="https://www.linkedin.com/in/wanhee-lee-31102820b/">Wanhee&nbsp;Lee</a>, 
              <a href="https://yunongliu.com/">Yunong&nbsp;Liu</a>,
              <a href="https://www.linkedin.com/in/jared-watrous/">Jared&nbsp;Watrous</a>, 
              <b>Stefan Stojanov</b>,
              <a href="https://www.niebles.net/">Juan Carlos&nbsp;Niebles</a>, 
              <a href="http://www.jiajunwu.com/">Jiajun&nbsp;Wu</a>,
              <a href="https://web.stanford.edu/~yamins/">Daniel&nbsp;Yamins</a>.
            </p>
            <p>preprint, 2025</p>
            <p><a href="https://arxiv.org/abs/2507.09082">arxiv</a> / <a href="https://neuroailab.github.io/projects/kl_tracing/">project page</a></p>
          </div>
        </div>
      </div>

      <hr />

      <div class="publication my-4">
        <div class="row justify-content-center align-items-center">
          <div class="col-auto p-0">
            <img src="img/pub/venkatesh_spelke_2025.gif" class="img-fluid" alt="" style="max-width: 150px" />
          </div>
          <div class="col-8">
            <p><b>Discovering and Using Spelke Segments</b></p>
            <p>Spelke segments (image regions that move together) emerge from counterfactual world models.</p>
            <p>
              <a href="https://rahulvenkk.github.io/">Rahul&nbsp;Venkatesh</a>,
              <a href="https://klemenkotar.github.io/">Klemen&nbsp;Kotar</a>,
              <a href="https://www.linkedin.com/in/lilian-chen-1975b81b1/">Lilian&nbsp;Naing&nbsp;Chen</a>,
              <a href="https://www.linkedin.com/in/seungwoo-simon-kim/">Seungwoo&nbsp;Kim</a>,
              <a href="https://www.linkedin.com/in/luca-wheeler-180818244/">Luca&nbsp;Thomas&nbsp;Wheeler</a>,
              <a href="https://www.linkedin.com/in/jared-watrous/">Jared&nbsp;Watrous</a>,
              <a href="https://www.linkedin.com/in/ashley-xu-a2588b1a9/">Ashley&nbsp;Xu</a>,
              <a href="https://www.linkedin.com/in/gia-ancone-58b545207/">Gia&nbsp;Ancone</a>,
              <a href="https://www.linkedin.com/in/wanhee-lee-31102820b/">Wanhee&nbsp;Lee</a>,
              <a href="https://www.linkedin.com/in/honglin-chen-52b13712a/">Honglin&nbsp;Chen</a>,
              <a href="https://www.linkedin.com/in/daniel-bear-b79480279/">Daniel&nbsp;Bear</a>,
              <b>Stefan&nbsp;Stojanov</b>,
              <a href="https://web.stanford.edu/~yamins/">Daniel&nbsp;Yamins</a>.
            </p>
            <p>preprint, 2025</p>
            <p><a href="https://arxiv.org/abs/2507.16038">arxiv</a> / <a href="https://neuroailab.github.io/spelke_net/">project page</a></p>
          </div>
        </div>
      </div>

      <hr />

      <div class="publication my-4">
        <div class="row justify-content-center align-items-center">
          <div class="col-auto p-0">
            <img src="img/pub/stojanov_optim_cfac_2025.gif" class="img-fluid" alt="" style="max-width: 150px" />
          </div>
          <div class="col-8">
            <p><b>Self-Supervised Learning of Motion Concepts by Optimizing Counterfactuals</b></p>
            <p>Optimizing visual prompts enables state-of-the-art motion estimation in videos.</p>
            <p>
              <b>Stefan&nbsp;Stojanov*</b>,
              <a href="https://www.linkedin.com/in/~davidwendt/">David&nbsp;Wendt*</a>,
              <a href="https://www.linkedin.com/in/seungwoo-simon-kim/">Seungwoo&nbsp;Kim*</a>,
              <a href="https://rahulvenkk.github.io/">Rahul&nbsp;Venkatesh*</a>,
              <a href="https://www.linkedin.com/in/kevin-feigelis/">Kevin&nbsp;Feigelis</a>,
              <a href="http://www.jiajunwu.com/">Jiajun&nbsp;Wu</a>,
              <a href="https://web.stanford.edu/~yamins/">Daniel&nbsp;Yamins</a>.
            </p>
            <p>preprint, 2025</p>
            <p>
               <a href="https://arxiv.org/abs/2503.19953">arxiv</a>
               /
               <a href="https://neuroailab.github.io/opt_cwm_page/">project page</a>
               /
               <a href="https://github.com/neuroailab/Opt_CWM/">code</a>
            </p>
          </div>
        </div>
      </div>

      <hr />

      <!-- Publication 4 -->
      <div class="publication my-4">
        <div class="row justify-content-center align-items-center">
          <div class="col-auto p-0">
            <img src="img/pub/stojanov_iccv_2025.jpg" class="img-fluid" alt="" style="max-width: 150px" />
          </div>
          <div class="col-8">
            <p><b>Weakly-Supervised Learning of Dense Functional Correspondences</b></p>
            <p>Pseudo-labels from VLMs plus spatial contrastive losses enable dense functional correspondences.</p>
            <p>
              <b>Stefan Stojanov*</b>,
              <a href="https://frankz24.github.io/">Linan&nbsp;Zhao*</a>,
              <a href="https://ai.stanford.edu/~yzzhang/">Yunzhi&nbsp;Zhang</a>,
              <a href="https://web.stanford.edu/~yamins/">Dan&nbsp;Yamins</a>,
              <a href="http://www.jiajunwu.com/">Jiajun&nbsp;Wu</a>.
            </p>
            <p>ICCV 2025 - poster</p>
            <p>
            <a href="https://dense-functional-correspondence.github.io/static/paper.pdf">paper</a>
            /
            <a href="https://dense-functional-correspondence.github.io">project page</a>
            </p>
          </div>
        </div>
      </div>

      <hr />
      
      <div class="publication my-4">
        <div class="row justify-content-center align-items-center">
          <div class="col-auto p-0">
            <img src="img/pub/long_ccn_2025.jpg" class="img-fluid" alt="" style="max-width: 150px" />
          </div>
          <div class="col-8">
            <p><b>The BabyView Dataset: High-resolution Egocentric Videos of Infants’ and Young Children’s Everyday Experiences</b></p>
            <p>A large egocentric dataset of infants and toddlers for vision and language learning.</p>
            <p>
              <a href="https://www.brialong.com/">Bria&nbsp;Long*</a>,
              <a href="https://rbzsparks.github.io/">Robert&nbsp;Z.&nbsp;Sparks*</a>,
              <a href="https://scholar.google.com/citations?user=1LQU1CQAAAAJ&hl=en">Violet&nbsp;Xiang*</a>,
              <b>Stefan&nbsp;Stojanov*</b>,
              <a href="https://yinzi.me/">Zi&nbsp;Yin</a>,
              <a href="https://bingschool.stanford.edu/people/grace-keene">Grace&nbsp;E.&nbsp;Keene</a>,
              <a href="https://alvinwmtan.github.io/">Alvin&nbsp;W.&nbsp;M.&nbsp;Tan</a>,
              <a href="https://styfeng.github.io/">Steven&nbsp;Y.&nbsp;Feng</a>,
              <a href="https://chengxuz.github.io/">Chengxu&nbsp;Zhuang</a>,
              <a href="https://scholar.google.com/citations?user=VPCX2XMAAAAJ&hl=en">Virginia&nbsp;A.&nbsp;Marchman</a>,
              <a href="https://web.stanford.edu/~yamins/">Daniel&nbsp;L.&nbsp;K.&nbsp;Yamins</a>,
              <a href="https://web.stanford.edu/~mcfrank/">Michael&nbsp;C.&nbsp;Frank</a>.
            </p>
            <p>CCN 2025 - poster</p>
            <p>
              <a href="https://arxiv.org/abs/2406.10447">arxiv</a>
            </p>
          </div>
        </div>
      </div>

      <hr />

      <div class="publication my-4">
        <div class="row justify-content-center align-items-center">
          <div class="col-auto p-0">
            <img src="img/pub/thai_eccv_2024.png" class="img-fluid" alt="3 × 2: 3D Object Part Segmentation by 2D Semantic Correspondences thumbnail" style="max-width: 150px" />
          </div>
          <div class="col-8">
            <p><b>3 × 2: 3D Object Part Segmentation by 2D Semantic Correspondences</b></p>
            <p>
              <a href="https://anhthai1997.wordpress.com/">Anh&nbsp;Thai</a>,
              Weiyao&nbsp;Wang,
              Hao&nbsp;Tang,
              <b>Stefan&nbsp;Stojanov</b>,
              <a href="https://rehg.org/">James&nbsp;M.&nbsp;Rehg</a>,
              Matt&nbsp;Feiszli.
            </p>
            <p>ECCV 2024 - poster</p>
            <p>
              <a href="https://arxiv.org/abs/2407.09648">paper</a>
            </p>
          </div>
        </div>
      </div>

      <hr />

      <div class="publication my-4">
        <div class="row justify-content-center align-items-center">
          <div class="col-auto p-0">
            <img src="img/pub/huang_cvpr_2024.png" class="img-fluid" alt="ZeroShape thumbnail" style="max-width: 150px" />
          </div>
          <div class="col-8">
            <p><b>ZeroShape: Regression-based Zero-shot Shape Reconstruction</b></p>
            <p>SOTA 3D shape reconstructor with high computational efficiency and low training data budget.</p>
            <p>
              <a href="https://zixuanh.com/">Zixuan&nbsp;Huang*</a>,
              <b>Stefan&nbsp;Stojanov*</b>,
              <a href="https://anhthai1997.wordpress.com/">Anh&nbsp;Thai</a>,
              <a href="https://varunjampani.github.io/">Varun&nbsp;Jampani</a>,
              <a href="https://rehg.org/">James&nbsp;M.&nbsp;Rehg</a>
            </p>
            <p>CVPR 2024 - poster</p>
            <p>
              <a href="https://arxiv.org/abs/2312.14198">paper</a>
              /
              <a href="https://github.com/zxhuang1698/ZeroShape">code</a>
              /
              <a href="https://zixuanh.com/projects/zeroshape.html">project page</a>
              /
              <a href="https://huggingface.co/spaces/zxhuang1698/ZeroShape">demo</a>
            </p>
          </div>
        </div>
      </div>

      <hr />

      <div class="publication my-4">
        <div class="row justify-content-center align-items-center">
          <div class="col-auto p-0">
            <img src="img/pub/thai_neurips_2023.gif" class="img-fluid" alt="Low-shot Object Learning thumbnail" style="max-width: 150px" />
          </div>
          <div class="col-8">
            <p><b>Low-shot Object Learning with Mutual Exclusivity Bias</b></p>
            <p>Mutual Exclusivity Bias enables fast learning of objects that generalizes.</p>
            <p>
              <a href="https://anhthai1997.wordpress.com/">Anh&nbsp;Thai</a>,
              <a href="http://ahumayun.com/">Ahmad&nbsp;Humayun</a>,
              <b>Stefan&nbsp;Stojanov</b>,
              <a href="https://zixuanh.com/">Zixuan&nbsp;Huang</a>,
              Bikram&nbsp;Boote,
              <a href="https://rehg.org/">James&nbsp;M.&nbsp;Rehg</a>
            </p>
            <p>NeurIPS 2023 – Datasets and Benchmarks Track</p>
            <p>
              <a href="https://arxiv.org/abs/2312.03533">paper</a>
              /
              <a href="https://github.com/rehg-lab/LSME">code</a>
              /
              <a href="https://ngailapdi.github.io/projects/lsme/">project page</a>
            </p>
          </div>
        </div>
      </div>

      <hr />

      <div class="publication my-4">
        <div class="row justify-content-center align-items-center">
          <div class="col-auto p-0">
            <img src="img/pub/huang_cvpr_2023.gif" class="img-fluid" alt="ShapeClipper thumbnail" style="max-width: 150px" />
          </div>
          <div class="col-8">
            <p><b>ShapeClipper: Scalable 3D Shape Learning via Geometric and CLIP-based Consistency</b></p>
            <p>CLIP and geometric consistency constraints facilitate scalable learning of object shape reconstruction.</p>
            <p>
              <a href="https://zixuanh.com/">Zixuan&nbsp;Huang</a>,
              <a href="https://varunjampani.github.io/">Varun&nbsp;Jampani</a>,
              <a href="https://anhthai1997.wordpress.com/">Anh&nbsp;Thai</a>,
              <a href="https://people.csail.mit.edu/yzli">Yuanzhen&nbsp;Li</a>,
              <b>Stefan&nbsp;Stojanov</b>,
              <a href="https://rehg.org/">James&nbsp;M.&nbsp;Rehg</a>
            </p>
            <p>CVPR 2023 – poster</p>
            <p>
              <a href="https://arxiv.org/abs/2304.06247">arxiv</a>
              /
              <a href="https://github.com/zxhuang1698/ShapeClipper">code</a>
              /
              <a href="https://zixuanh.com/projects/shapeclipper.html">project page</a>
              /
              <a href="https://www.youtube.com/watch?v=BxTGVjXoXu8">video</a>
            </p>
          </div>
        </div>
      </div>

      <hr />

      <div class="publication my-4">
        <div class="row justify-content-center align-items-center">
          <div class="col-auto p-0">
            <img src="img/pub/stojanov_neurips_2022.png" class="img-fluid" alt="Learning Dense Object Descriptors thumbnail" style="max-width: 150px" />
          </div>
          <div class="col-8">
            <p><b>Learning Dense Object Descriptors from Multiple Views for Low-shot Category Generalization</b></p>
            <p>Dense feature-level self-supervised learning from multiple camera views without any category labels leads to representations that can generalize to novel categories.</p>
            <p>
              <b>Stefan&nbsp;Stojanov</b>,
              <a href="https://anhthai1997.wordpress.com/">Anh&nbsp;Thai</a>,
              <a href="https://zixuanh.com/">Zixuan&nbsp;Huang</a>,
              <a href="https://rehg.org/">James&nbsp;M.&nbsp;Rehg</a>
            </p>
            <p>NeurIPS 2022 – poster</p>
            <p>
              <a href="https://arxiv.org/abs/2211.15059">arxiv</a>
              /
              <a href="https://github.com/rehg-lab/dope_selfsup">code</a>
              /
              <a href="https://sstojanov.github.io/projects/dope_selfsup.html">project page</a>
              /
              <a href="https://sstojanov.github.io/assets/neurips2022/poster.pdf">poster</a>
              /
              <a href="https://www.youtube.com/watch?v=qaArkLiiymkv">video</a>
            </p>
          </div>
        </div>
      </div>

      <hr />

      <div class="publication my-4">
        <div class="row justify-content-center align-items-center">
          <div class="col-auto p-0">
            <img src="img/pub/huang_eccv_2022.jpg" class="img-fluid" alt="Planes vs. Chairs thumbnail" style="max-width: 150px" />
          </div>
          <div class="col-8">
            <p><b>Planes vs. Chairs: Category-guided 3D Shape Learning without any 3D Cues</b></p>
            <p>A 3D-unsupervised model that learns shapes of multiple object categories at once.</p>
            <p>
              <a href="https://zixuanh.com/">Zixuan&nbsp;Huang</a>,
              <b>Stefan&nbsp;Stojanov</b>,
              <a href="https://anhthai1997.wordpress.com/">Anh&nbsp;Thai</a>,
              <a href="https://varunjampani.github.io/">Varun&nbsp;Jampani</a>,
              <a href="https://rehg.org/">James&nbsp;M.&nbsp;Rehg</a>
            </p>
            <p>ECCV 2022 – poster</p>
            <p>
              <a href="https://arxiv.org/abs/2204.10235">arxiv</a>
              /
              <a href="https://github.com/zxhuang1698/cat-3d">code</a>
              /
              <a href="https://zixuanh.com/eccv2022-multiclass3D.html">project page</a>
              /
              <a href="https://zixuanh.com/eccv2022-multiclass3D/4231.pdf">poster</a>
              /
              <a href="https://zixuanh.com/eccv2022-multiclass3D/4231.mp4">video</a>
            </p>
          </div>
        </div>
      </div>

      <hr />

      <div class="publication my-4">
        <div class="row justify-content-center align-items-center">
          <div class="col-auto p-0">
            <img src="img/pub/thai_3dv_2022.png" class="img-fluid" alt="Continual 3D Shape Reconstruction thumbnail" style="max-width: 150px" />
          </div>
          <div class="col-8">
            <p><b>The Surprising Positive Knowledge Transfer in Continual 3D Object Shape Reconstruction</b></p>
            <p>Continual learning of 3D shape reconstruction does not suffer from catastrophic forgetting as much as discriminative learning tasks.</p>
            <p>
              <a href="https://anhthai1997.wordpress.com/">Anh&nbsp;Thai</a>,
              <b>Stefan&nbsp;Stojanov</b>,
              <a href="https://zixuanh.com/">Zixuan&nbsp;Huang</a>,
              <a href="https://rehg.org/">James&nbsp;M.&nbsp;Rehg</a>
            </p>
            <p>3DV 2022 – poster</p>
            <p>
              <a href="https://arxiv.org/abs/2101.07295">arxiv</a>
              /
              <a href="https://github.com/rehg-lab/CLRec">code</a>
            </p>
          </div>
        </div>
      </div>

      <hr />

      <div class="publication my-4">
        <div class="row justify-content-center align-items-center">
          <div class="col-auto p-0">
            <img src="img/pub/stojanov_etra_2022.png" class="img-fluid" alt="Depth for Gaze Estimation thumbnail" style="max-width: 150px" />
          </div>
          <div class="col-8">
            <p><b>The Benefits of Depth Information for Head-Mounted Gaze Estimation</b></p>
            <p>Fusing depth and image information improves deep models' robustness to fitment and slip for head-mounted gaze estimation.</p>
            <p>
              <b>Stefan&nbsp;Stojanov</b>,
              <a href="https://scholar.google.com/citations?user=Tz_jwsQAAAAJ&amp;hl=en">Sachin&nbsp;Talathi</a>,
              <a href="https://scholar.google.com/citations?user=18fTep8AAAAJ&amp;hl=en">Abhishek&nbsp;Sharma</a>
            </p>
            <p>ETRA 2022 – short paper</p>
            <p>
              <a href="https://dl.acm.org/doi/pdf/10.1145/3517031.3529638">pdf</a>
            </p>
          </div>
        </div>
      </div>

      <hr />

      <div class="publication my-4">
        <div class="row justify-content-center align-items-center">
          <div class="col-auto p-0">
            <img src="img/pub/stojanov_cvpr_2021.png" class="img-fluid" alt="Using Shape to Categorize thumbnail" style="max-width: 150px" />
          </div>
          <div class="col-8">
            <p><b>Using Shape to Categorize: Low-Shot Learning with an Explicit Shape Bias</b></p>
            <p>Learning representations to generalize based on 3D shape and then learning to map images into them leads to improved low-shot generalization.</p>
            <p>
              <b>Stefan&nbsp;Stojanov</b>,
              <a href="https://anhthai1997.wordpress.com/">Anh&nbsp;Thai</a>,
              <a href="https://rehg.org/">James&nbsp;M.&nbsp;Rehg</a>
            </p>
            <p>CVPR 2021 – poster</p>
            <p>
              <a href="https://arxiv.org/abs/2101.07296">arxiv</a>
              /
              <a href="https://rehg-lab.github.io/publication-pages/lowshot-shapebias/">code</a>
              /
              <a href="https://github.com/rehg-lab/lowshot-shapebias/tree/main/toys4k">dataset</a>
              /
              <a href="https://rehg-lab.github.io/publication-pages/lowshot-shapebias/">project page</a>
            </p>
          </div>
        </div>
      </div>

      <hr />

      <div class="publication my-4">
        <div class="row justify-content-center align-items-center">
          <div class="col-auto p-0">
            <img src="img/pub/thai_3dv_2021.png" class="img-fluid" alt="3D Reconstruction from Single Images thumbnail" style="max-width: 150px" />
          </div>
          <div class="col-8">
            <p><b>3D Reconstruction of Novel Object Shapes from Single Images</b></p>
            <p>An implicit SDF representation-based method for single-view 3D shape reconstruction.</p>
            <p>
              <a href="https://anhthai1997.wordpress.com/">Anh&nbsp;Thai</a>*,
              <b>Stefan&nbsp;Stojanov</b>*,
              <a href="https://rehg.org/">James&nbsp;M.&nbsp;Rehg</a>
            </p>
            <p>3DV 2021 – oral</p>
            <p>
              <a href="https://arxiv.org/abs/2006.07752">arxiv</a>
              /
              <a href="https://github.com/rehg-lab/3DShapeGen">code</a>
              /
              <a href="https://devlearning-gt.github.io/3DShapeGen/">project page</a>
            </p>
          </div>
        </div>
      </div>

      <hr />

      <div class="publication my-4">
        <div class="row justify-content-center align-items-center">
          <div class="col-auto p-0">
            <img src="img/pub/stojanov_cvpr_2019.png" class="img-fluid" alt="Incremental Object Learning thumbnail" style="max-width: 150px" />
          </div>
          <div class="col-8">
            <p><b>Incremental Object Learning from Contiguous Views</b></p>
            <p>Repetition of learned concepts in continual learning ameliorates catastrophic forgetting.</p>
            <p>
              <b>Stefan&nbsp;Stojanov</b>,
              <a href="https://anhthai1997.wordpress.com/">Anh&nbsp;Thai</a>*,
              <a href="https://scholar.google.com/citations?user=Vxk4TM4AAAAJ&amp;hl=en">Samarth&nbsp;Mishra</a>*,
              <a href="https://rehg.org/">James&nbsp;M.&nbsp;Rehg</a>
            </p>
            <p>CVPR 2019 – oral – <b>Best Paper Award Finalist</b></p>
            <p>
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Stojanov_Incremental_Object_Learning_From_Contiguous_Views_CVPR_2019_paper.pdf">pdf</a>
              /
              <a href="https://github.com/iolfcv/experiments">code</a>
              /
              <a href="https://github.com/iolfcv/CRIB_Data_Generator">dataset</a>
              /
              <a href="https://www.youtube.com/watch?v=0lybuew8Lik&amp;t=3860s">video</a>
            </p>
          </div>
        </div>
      </div>

      <hr />

      <div class="publication my-4">
        <div class="row justify-content-center align-items-center">
          <div class="col-auto p-0">
            <img src="img/pub/chen_cvpr_2019.png" class="img-fluid" alt="Unsupervised 3D Pose Estimation thumbnail" style="max-width: 150px" />
          </div>
          <div class="col-8">
            <p><b>Unsupervised 3D Pose Estimation with Geometric Self-supervision</b></p>
            <p>Utilizing adversarial learning to estimate 3D human pose without 3D supervision.</p>
            <p>
              <a href="https://scholar.google.com/citations?user=GlZioF6IAMYC&amp;hl=en">Ching-Hang&nbsp;Chen</a>,
              <a href="https://scholar.google.com/citations?user=GaSWCoUAAAAJ&amp;hl=en">Ambrish&nbsp;Tyagi</a>,
              <a href="https://scholar.google.com/citations?user=Q3puGtcAAAAJ&amp;hl=en">Amit&nbsp;Agrawal</a>,
              <a href="https://scholar.google.com/citations?user=iYipS7kAAAAJ&amp;hl=en">Dylan&nbsp;Drover</a>,
              <b>Stefan&nbsp;Stojanov</b>,
              <a href="https://rehg.org/">James&nbsp;M.&nbsp;Rehg</a>
            </p>
            <p>CVPR 2019 – poster</p>
            <p>
              <a href="https://arxiv.org/abs/1904.04812">arxiv</a>
            </p>
          </div>
        </div>
      </div>

    </div>

    <!-- Footer -->
    <div class="container-md p-2 my-3 border text-right small">
      Design inspired by the websites of
      <a href="https://gkioxari.github.io/">Georgia Gkioxari</a>
      and
      <a href="https://jonbarron.info/">Jon Barron</a>
    </div>
  </body>
</html>

